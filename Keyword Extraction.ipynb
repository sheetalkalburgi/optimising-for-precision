{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statsmodels.api as sm # We can fit logistic regression using statsmodel as well\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(src, tgt):\n",
    "    similarity = cosine_similarity(src, tgt)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3225, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FDA_Chapter</th>\n",
       "      <th>FDA_Section</th>\n",
       "      <th>FDA_Code</th>\n",
       "      <th>FDA_Subpart</th>\n",
       "      <th>FDA_Description</th>\n",
       "      <th>matched</th>\n",
       "      <th>score</th>\n",
       "      <th>HC_Code</th>\n",
       "      <th>HC_Chapter</th>\n",
       "      <th>HC_Section</th>\n",
       "      <th>HC_Subpart</th>\n",
       "      <th>HC_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENERAL ENFORCEMENT REGULATIONS</td>\n",
       "      <td>General Provisions</td>\n",
       "      <td>\"1.3\"</td>\n",
       "      <td>Definitions.</td>\n",
       "      <td>(a)  Labeling   includes all written, printed,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635591</td>\n",
       "      <td>C.01.031</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Cautionary Statements and Child Resistant Pack...</td>\n",
       "      <td>Cautionary Statements and Child Resistant Pack...</td>\n",
       "      <td>(1) Subject to section C.01.031.2, (a) no pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GENERAL ENFORCEMENT REGULATIONS</td>\n",
       "      <td>General Labeling Requirements</td>\n",
       "      <td>\"1.20\"</td>\n",
       "      <td>Presence of mandatory label information.</td>\n",
       "      <td>In the regulations specified in 1.1(c) of this...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.779747</td>\n",
       "      <td>C.03.208</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Good Manufacturing Practices</td>\n",
       "      <td>Drugs, Other than Radionuclides, Sold or Repre...</td>\n",
       "      <td>Every kit shall be labelled to show (a) its pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GENERAL ENFORCEMENT REGULATIONS</td>\n",
       "      <td>General Labeling Requirements</td>\n",
       "      <td>\"1.21\"</td>\n",
       "      <td>Failure to reveal material facts.</td>\n",
       "      <td>(a) Labeling of a food, drug, device, cosmetic...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.729136</td>\n",
       "      <td>C.01.029</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Cautionary Statements and Child Resistant Pack...</td>\n",
       "      <td>Cautionary Statements and Child Resistant Pack...</td>\n",
       "      <td>(1) Subject to subsections C.01.031.2(1) and (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       FDA_Chapter                    FDA_Section FDA_Code  \\\n",
       "0  GENERAL ENFORCEMENT REGULATIONS             General Provisions    \"1.3\"   \n",
       "1  GENERAL ENFORCEMENT REGULATIONS  General Labeling Requirements   \"1.20\"   \n",
       "2  GENERAL ENFORCEMENT REGULATIONS  General Labeling Requirements   \"1.21\"   \n",
       "\n",
       "                                FDA_Subpart  \\\n",
       "0                              Definitions.   \n",
       "1  Presence of mandatory label information.   \n",
       "2         Failure to reveal material facts.   \n",
       "\n",
       "                                     FDA_Description  matched     score  \\\n",
       "0  (a)  Labeling   includes all written, printed,...        1  0.635591   \n",
       "1  In the regulations specified in 1.1(c) of this...        1  0.779747   \n",
       "2  (a) Labeling of a food, drug, device, cosmetic...        1  0.729136   \n",
       "\n",
       "    HC_Code HC_Chapter                                         HC_Section  \\\n",
       "0  C.01.031      Drugs  Cautionary Statements and Child Resistant Pack...   \n",
       "1  C.03.208      Drugs                       Good Manufacturing Practices   \n",
       "2  C.01.029      Drugs  Cautionary Statements and Child Resistant Pack...   \n",
       "\n",
       "                                          HC_Subpart  \\\n",
       "0  Cautionary Statements and Child Resistant Pack...   \n",
       "1  Drugs, Other than Radionuclides, Sold or Repre...   \n",
       "2  Cautionary Statements and Child Resistant Pack...   \n",
       "\n",
       "                                      HC_Description  \n",
       "0  (1) Subject to section C.01.031.2, (a) no pers...  \n",
       "1  Every kit shall be labelled to show (a) its pr...  \n",
       "2  (1) Subject to subsections C.01.031.2(1) and (...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel('drugs.xlsx')\n",
    "test_Interpretation= test.loc[test['HC_Subpart'] == 'Interpretation']\n",
    "test_General = test.loc[test['HC_Subpart'] == 'General']\n",
    "test_removal = pd.concat([test_Interpretation,test_General])\n",
    "cond = test['HC_Subpart'].isin(test_removal['HC_Subpart'])\n",
    "test.drop(test[cond].index, inplace = True)\n",
    "test.reset_index(drop=True, inplace=True)\n",
    "print(test.shape)\n",
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FDA_Chapter</th>\n",
       "      <th>FDA_Section</th>\n",
       "      <th>FDA_Code</th>\n",
       "      <th>FDA_Subpart</th>\n",
       "      <th>FDA_Description</th>\n",
       "      <th>matched</th>\n",
       "      <th>score</th>\n",
       "      <th>HC_Code</th>\n",
       "      <th>HC_Chapter</th>\n",
       "      <th>HC_Section</th>\n",
       "      <th>HC_Subpart</th>\n",
       "      <th>HC_Description</th>\n",
       "      <th>Yes(y) / No(n)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CURRENT GOOD MANUFACTURING PRACTICE FOR FINISH...</td>\n",
       "      <td>Buildings and Facilities</td>\n",
       "      <td>\"211.56\"</td>\n",
       "      <td>Sanitation.</td>\n",
       "      <td>(a) Any building used in the manufacture, proc...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.773877</td>\n",
       "      <td>C.02.004</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Good Manufacturing Practices</td>\n",
       "      <td>Premises</td>\n",
       "      <td>The premises in which a lot or batch of a drug...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CURRENT GOOD MANUFACTURING PRACTICE FOR TYPE A...</td>\n",
       "      <td>Construction and Maintenance of Facilities and...</td>\n",
       "      <td>\"226.20\"</td>\n",
       "      <td>Buildings.</td>\n",
       "      <td>Buildings in which Type A medicated article(s)...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.724563</td>\n",
       "      <td>C.02.004</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Good Manufacturing Practices</td>\n",
       "      <td>Premises</td>\n",
       "      <td>The premises in which a lot or batch of a drug...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CURRENT GOOD MANUFACTURING PRACTICE FOR FINISH...</td>\n",
       "      <td>Buildings and Facilities</td>\n",
       "      <td>\"211.58\"</td>\n",
       "      <td>Maintenance.</td>\n",
       "      <td>Any building used in the manufacture, processi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.717186</td>\n",
       "      <td>C.02.004</td>\n",
       "      <td>Drugs</td>\n",
       "      <td>Good Manufacturing Practices</td>\n",
       "      <td>Premises</td>\n",
       "      <td>The premises in which a lot or batch of a drug...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         FDA_Chapter  \\\n",
       "0  CURRENT GOOD MANUFACTURING PRACTICE FOR FINISH...   \n",
       "1  CURRENT GOOD MANUFACTURING PRACTICE FOR TYPE A...   \n",
       "2  CURRENT GOOD MANUFACTURING PRACTICE FOR FINISH...   \n",
       "\n",
       "                                         FDA_Section  FDA_Code   FDA_Subpart  \\\n",
       "0                           Buildings and Facilities  \"211.56\"   Sanitation.   \n",
       "1  Construction and Maintenance of Facilities and...  \"226.20\"    Buildings.   \n",
       "2                           Buildings and Facilities  \"211.58\"  Maintenance.   \n",
       "\n",
       "                                     FDA_Description  matched     score  \\\n",
       "0  (a) Any building used in the manufacture, proc...        1  0.773877   \n",
       "1  Buildings in which Type A medicated article(s)...        1  0.724563   \n",
       "2  Any building used in the manufacture, processi...        1  0.717186   \n",
       "\n",
       "    HC_Code HC_Chapter                    HC_Section HC_Subpart  \\\n",
       "0  C.02.004      Drugs  Good Manufacturing Practices   Premises   \n",
       "1  C.02.004      Drugs  Good Manufacturing Practices   Premises   \n",
       "2  C.02.004      Drugs  Good Manufacturing Practices   Premises   \n",
       "\n",
       "                                      HC_Description Yes(y) / No(n)  \n",
       "0  The premises in which a lot or batch of a drug...              Y  \n",
       "1  The premises in which a lot or batch of a drug...              Y  \n",
       "2  The premises in which a lot or batch of a drug...              Y  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_excel('fda_ hc RVP review 11172020.xlsx')\n",
    "train.drop(['Unnamed: 13'],axis=1,inplace=True)\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "print(train.shape)\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_lst = [\"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"ain\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"aren\",\"aren't\",\"as\",\n",
    "       \"at\",\"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\"can\",\"couldn\",\"couldn't\",\n",
    "       \"d\",\"did\",\"didn\",\"didn't\",\"do\",\"does\",\"doesn\",\"doesn't\",\"doing\",\"don\",\"don't\",\"down\",\"during\",\n",
    "       \"each\",\"few\",\"for\",\"from\",\"further\",\"had\",\"hadn\",\"hadn't\",\"has\",\"hasn\",\"hasn't\",\"have\",\"haven\",\n",
    "       \"haven't\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"i\",\"if\",\"in\",\n",
    "       \"into\",\"is\",\"isn\",\"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"just\",\"ll\",\"m\",\"ma\",\"me\",\"mightn\",\"mightn't\",\n",
    "       \"more\",\"most\",\"mustn\",\"mustn't\",\"my\",\"myself\",\"needn\",\"needn't\",\"no\",\"nor\",\"not\",\"now\",\"o\",\"of\",\"off\",\"on\",\n",
    "       \"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\"re\",\"s\",\"same\",\"shan\",\"shan't\",\n",
    "       \"she\",\"she's\",\"should\",\"should've\",\"shouldn\",\"shouldn't\",\"so\",\"some\",\"such\",\"t\",\"than\",\"that\",\"that'll\",\n",
    "       \"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\"this\",\"those\",\"through\",\"to\",\"too\",\n",
    "       \"under\",\"until\",\"up\",\"ve\",\"very\",\"was\",\"wasn\",\"wasn't\",\"we\",\"were\",\"weren\",\"weren't\",\"what\",\"when\",\"where\",\n",
    "       \"which\",\"while\",\"who\",\"whom\",\"why\",\"will\",\"with\",\"won\",\"won't\",\"wouldn\",\"wouldn't\",\"y\",\"you\",\"you'd\",\"you'll\",\n",
    "       \"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"could\",\"he'd\",\"he'll\",\"he's\",\"here's\",\"how's\",\"i'd\",\n",
    "       \"i'll\",\"i'm\",\"i've\",\"let's\",\"ought\",\"she'd\",\"shall\",\"she'll\",\"that's\",\"there's\",\"they'd\",\"they'll\",\"they're\",\"they've\",\n",
    "       \"we'd\",\"we'll\",\"we're\",\"we've\",\"what's\",\"when's\",\"where's\",\"who's\",\"why's\",\"would\",\"able\",\"abst\",\n",
    "       \"accordance\",\"according\",\"accordingly\",\"across\",\"act\",\"actually\",\"added\",\"adj\",\"affected\",\"affecting\",\n",
    "       \"affects\",\"afterwards\",\"ah\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\n",
    "       \"announce\",\"another\",\"anybody\",\"anyhow\",\"anymore\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apparently\",\n",
    "       \"approximately\",\"arent\",\"arise\",\"around\",\"aside\",\"ask\",\"asking\",\"auth\",\"available\",\"away\",\"awfully\",\"b\",\"back\",\n",
    "       \"became\",\"become\",\"becomes\",\"becoming\",\"beforehand\",\"begin\",\"beginning\",\"beginnings\",\"begins\",\"behind\",\"believe\",\n",
    "       \"beside\",\"besides\",\"beyond\",\"biol\",\"brief\",\"briefly\",\"c\",\"ca\",\"came\",\"cannot\",\"can't\",\"cause\",\"causes\",\"certain\",\n",
    "       \"certainly\",\"co\",\"com\",\"come\",\"comes\",\"contain\",\"containing\",\"contains\",\"couldnt\",\"date\",\"different\",\"done\",\n",
    "       \"downwards\",\"due\",\"e\",\"ed\",\"edu\",\"effect\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ending\",\"enough\",\n",
    "       \"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"except\",\"f\",\n",
    "       \"far\",\"ff\",\"fifth\",\"first\",\"five\",\"fix\",\"followed\",\"following\",\"follows\",\"former\",\"formerly\",\"forth\",\"found\",\"four\",\"furthermore\",\"g\",\"gave\",\n",
    "       \"get\",\"gets\",\"getting\",\"give\",\"given\",\"gives\",\"giving\",\"go\",\"goes\",\"gone\",\"got\",\"gotten\",\"h\",\"happens\",\"hardly\",\"hed\",\"hence\",\"hereafter\",\n",
    "             \"hereby\",\"herein\",\"heres\",\"hereupon\",\"hes\",\"hi\",\"hid\",\"hither\",\"home\",\"howbeit\",\"however\",\"hundred\",\"id\",\"ie\",\"im\",\"inc\",\"indeed\",\"index\",\"instead\",\"invention\",\"inward\",\n",
    "             \"itd\",\"it'll\",\"j\",\"k\",\"keep\",\"keeps\",\"kept\",\"kg\",\"km\",\"know\",\"known\",\"knows\",\"l\",\"largely\",\"last\",\"lately\",\"later\",\"latter\",\n",
    "             \"latterly\",\"least\",\"less\",\"lest\",\"let\",\"lets\",\"like\",\"liked\",\"likely\",\"line\",\"little\",\"'ll\",\"look\",\n",
    "             \"looking\",\"looks\",\"ltd\",\"made\",\"mainly\",\"make\",\"makes\",\"many\",\"may\",\"maybe\",\"mean\",\"means\",\"meantime\",\n",
    "             \"meanwhile\",\"merely\",\"mg\",\"might\",\"million\",\"miss\",\"ml\",\"moreover\",\"mostly\",\"mr\",\"mrs\",\"much\",\"mug\",\n",
    "             \"must\",\"n\",\"na\",\"name\",\"namely\",\"nay\",\"nd\",\"near\",\"nearly\",\"necessarily\",\"necessary\",\"need\",\"needs\",\n",
    "             \"neither\",\"never\",\"nevertheless\",\"next\",\"nine\",\"ninety\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\n",
    "             \"normally\",\"nos\",\"noted\",\"nothing\",\"nowhere\",\"obtain\",\"obtained\",\"obviously\",\"often\",\"oh\",\"ok\",\"okay\",\n",
    "             \"old\",\"omitted\",\"one\",\"ones\",\"onto\",\"ord\",\"others\",\"otherwise\",\"outside\",\"overall\",\"owing\",\"p\",\"page\",\n",
    "             \"pages\",\"part\",\"particular\",\"particularly\",\"past\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\n",
    "             \"possibly\",\"potentially\",\"pp\",\"predominantly\",\"present\",\"previously\",\"primarily\",\"probably\",\n",
    "             \"promptly\",\"proud\",\"provides\",\"put\",\"q\",\"que\",\"quickly\",\"quite\",\"qv\",\"r\",\"ran\",\"rather\",\"rd\",\"readily\",\n",
    "             \"really\",\"recent\",\"recently\",\"ref\",\"refs\",\"regarding\",\"regardless\",\"regards\",\"related\",\"relatively\",\"research\",\n",
    "             \"respectively\",\"resulted\",\"resulting\",\"right\",\"run\",\"said\",\"saw\",\"say\",\"saying\",\"says\",\"sec\",\"section\",\"see\",\n",
    "             \"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sent\",\"seven\",\"several\",\"shall\",\"shed\",\"shes\",\n",
    "             \"show\",\"showed\",\"shown\",\"showns\",\"shows\",\"significantly\",\"similarly\",\"since\",\"six\",\"slightly\",\"somebody\",\"somehow\",\n",
    "             \"someone\",\"somethan\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specifically\",\"specified\",\"specify\",\"specifying\",\"still\",\"stop\",\"strongly\",\"sub\",\"substantially\",\"successfully\",\"sufficiently\",\"suggest\",\"sup\",\"sure\",\"take\",\"taken\",\"taking\",\"tell\",\"tends\",\"th\",\"thank\",\"thanks\",\n",
    "             \"thanx\",\"thats\",\"that've\",\"thence\",\"thereafter\",\"thereby\",\"thered\",\"therefore\",\"therein\",\"there'll\",\"thereof\",\"therere\",\"theres\",\"thereto\",\"thereupon\",\"there've\",\"theyd\",\"theyre\",\"think\",\"thou\",\"though\",\"thoughh\",\"thousand\",\"throug\",\"throughout\",\"thru\",\"thus\",\"til\",\"tip\",\"together\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"ts\",\"twice\",\"two\",\"u\",\"un\",\"unfortunately\",\"unless\",\"unlike\",\"unlikely\",\"unto\",\"upon\",\"ups\",\"us\",\"use\",\"used\",\"useful\",\"usefully\",\"usefulness\",\"uses\",\"using\",\"usually\",\"v\",\"value\",\"various\",\"'ve\",\"via\",\"viz\",\"vol\",\"vols\",\"vs\",\"w\",\"want\",\"wants\",\"wasnt\",\"way\",\"wed\",\"welcome\",\"went\",\"werent\",\"whatever\",\"what'll\",\"whats\",\"whence\",\"whenever\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"wheres\",\"whereupon\",\"wherever\",\"whether\",\"whim\",\"whither\",\"whod\",\"whoever\",\"whole\",\"who'll\",\"whomever\",\"whos\",\"whose\",\"widely\",\"willing\",\"wish\",\"within\",\"without\",\"wont\",\"words\",\"world\",\"wouldnt\",\"www\",\"x\",\"yes\",\"yet\",\"youd\",\"youre\",\"z\",\"zero\",\"a's\",\"ain't\",\"allow\",\"allows\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"associated\",\"best\",\"better\",\"c'mon\",\"c's\",\"cant\",\"changes\",\"clearly\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"corresponding\",\"course\",\"currently\",\"definitely\",\"described\",\"despite\",\"entirely\",\"exactly\",\"example\",\"going\",\"greetings\",\"hello\",\"help\",\"hopefully\",\"ignored\",\"inasmuch\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"it'd\",\"keep\",\"keeps\",\"novel\",\"presumably\",\"reasonably\",\"second\",\"secondly\",\"sensible\",\"serious\",\"seriously\",\"sure\",\"t's\",\"third\",\"thorough\",\"thoroughly\",\"three\",\"well\",\"wonder\",\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\",\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\",\n",
    "\"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\",\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\",'co','op','research-articl', 'pagecount','cit','ibid','les','le','au','que','est','pas','vol','el','los','pp','u201d','well-b', 'http', 'volumtype', 'par', '0o', '0s', '3a', '3b', '3d', '6b', '6o', 'a1', 'a2', 'a3', 'a4', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'aj', 'al', 'an', 'ao', 'ap', 'ar', 'av', 'aw', 'ax', 'ay', 'az', 'b1', 'b2', 'b3', 'ba', 'bc', 'bd', 'be', 'bi', 'bj', 'bk', 'bl', 'bn', 'bp', 'br', 'bs', 'bt', 'bu', 'bx', 'c1', 'c2', 'c3', 'cc', 'cd', 'ce', 'cf', 'cg', 'ch', 'ci', 'cj', 'cl', 'cm', 'cn', 'cp', 'cq', 'cr', 'cs', 'ct', 'cu', 'cv', 'cx', 'cy', 'cz', 'd2', 'da', 'dc', 'dd', 'de', 'df', 'di', 'dj', 'dk', 'dl', 'do', 'dp', 'dr', 'ds', 'dt', 'du', 'dx', 'dy', 'e2', 'e3', 'ea', 'ec', 'ed', 'ee', 'ef', 'ei', 'ej', 'el', 'em', 'en', 'eo', 'ep', 'eq', 'er', 'es', 'et', 'eu', 'ev', 'ex', 'ey', 'f2', 'fa', 'fc', 'ff', 'fi', 'fj', 'fl', 'fn', 'fo', 'fr', 'fs', 'ft', 'fu', 'fy', 'ga', 'ge', 'gi', 'gj', 'gl', 'go', 'gr', 'gs', 'gy', 'h2', 'h3', 'hh', 'hi', 'hj', 'ho', 'hr', 'hs', 'hu', 'hy', 'i', 'i2', 'i3', 'i4', 'i6', 'i7', 'i8', 'ia', 'ib', 'ic', 'ie', 'ig', 'ih', 'ii', 'ij', 'il', 'in', 'io', 'ip', 'iq', 'ir', 'iv', 'ix', 'iy', 'iz', 'jj', 'jr', 'js', 'jt', 'ju', 'ke', 'kg', 'kj', 'km', 'ko', 'l2', 'la', 'lb', 'lc', 'lf', 'lj', 'ln', 'lo', 'lr', 'ls', 'lt', 'm2', 'ml', 'mn', 'mo', 'ms', 'mt', 'mu', 'n2', 'nc', 'nd', 'ne', 'ng', 'ni', 'nj', 'nl', 'nn', 'nr', 'ns', 'nt', 'ny', 'oa', 'ob', 'oc', 'od', 'of', 'og', 'oi', 'oj', 'ol', 'om', 'on', 'oo', 'oq', 'or', 'os', 'ot', 'ou', 'ow', 'ox', 'oz', 'p1', 'p2', 'p3', 'pc', 'pd', 'pe', 'pf', 'ph', 'pi', 'pj', 'pk', 'pl', 'pm', 'pn', 'po', 'pq', 'pr', 'ps', 'pt', 'pu', 'py', 'qj', 'qu', 'r2', 'ra', 'rc', 'rd', 'rf', 'rh', 'ri', 'rj', 'rl', 'rm', 'rn', 'ro', 'rq', 'rr', 'rs', 'rt', 'ru', 'rv', 'ry', 's2', 'sa', 'sc', 'sd', 'se', 'sf', 'si', 'sj', 'sl', 'sm', 'sn', 'sp', 'sq', 'sr', 'ss', 'st', 'sy', 'sz', 't1', 't2', 't3', 'tb', 'tc', 'td', 'te', 'tf', 'th', 'ti', 'tj', 'tl', 'tm', 'tn', 'tp', 'tq', 'tr', 'ts', 'tt', 'tv', 'tx', 'ue', 'ui', 'uj', 'uk', 'um', 'un', 'uo', 'ur', 'ut', 'va', 'wa', 'vd', 'wi', 'vj', 'vo', 'wo', 'vq', 'vt', 'vu', 'x1', 'x2', 'x3', 'xf', 'xi', 'xj', 'xk', 'xl', 'xn', 'xo', 'xs', 'xt', 'xv', 'xx', 'y2', 'yj', 'yl', 'yr', 'ys', 'yt', 'zi', 'zz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fda_desc_cleaned']=''\n",
    "train['hc_desc_cleaned']=''\n",
    "\n",
    "# Pre-processing FDA Descriptions\n",
    "lemmatizer = WordNetLemmatizer() # stem instead\n",
    "pattern1 = re.compile(r'\\b(' + r'|'.join(stopwords_lst) + r')\\b\\s*')\n",
    "pattern2 = '[0-9]'\n",
    "for i in range(0,len(train)):\n",
    "        sent_ref = train.iloc[i]['FDA_Description']\n",
    "#         sent_ref = sent_ref.lower() # convert to lower case\n",
    "        word_list = nltk.word_tokenize(sent_ref)\n",
    "        sent_ref = ' '.join([lemmatizer.lemmatize(w) for w in word_list]) # lemmatize\n",
    "        sent_ref = re.sub(r'\\([^)]*\\)', '', sent_ref) # remove characters which are within () and []\n",
    "        sent_ref = pattern1.sub('', sent_ref) # remove stopwords\n",
    "        sent_ref = re.sub(r'[^\\w\\s]','',sent_ref) # remove spaces (new line, tabs etc)\n",
    "        sent_ref = re.sub(pattern2, '', sent_ref) # remove numbers\n",
    "        sent_ref = \" \".join(sent_ref.split()) # remove whitespaces, if any\n",
    "        train['fda_desc_cleaned'][i] = sent_ref\n",
    "        \n",
    "# Pre-processing Health Canada Descriptions\n",
    "for i in range(0,len(train)):\n",
    "        sent_ref = train.iloc[i]['HC_Description']\n",
    "#         sent_ref = sent_ref.lower() # convert to lower case\n",
    "        word_list = nltk.word_tokenize(sent_ref)\n",
    "        sent_ref = ' '.join([lemmatizer.lemmatize(w) for w in word_list]) # lemmatize\n",
    "        sent_ref = re.sub(r'\\([^)]*\\)', '', sent_ref) # remove characters which are within parenthesis\n",
    "        sent_ref = pattern1.sub('', sent_ref) # remove stopwords\n",
    "        sent_ref = re.sub(r'[^\\w\\s]','',sent_ref) # remove spaces (new line, tabs etc)\n",
    "        sent_ref = re.sub(pattern2, '', sent_ref) # remove numbers\n",
    "        sent_ref = \" \".join(sent_ref.split()) # remove whitespaces, if any\n",
    "        train['hc_desc_cleaned'][i] = sent_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN','VERB', 'ADJ'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pair text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = train['fda_desc_cleaned'][0]\n",
    "sent2 = train['hc_desc_cleaned'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(a) Any building used in the manufacture, processing, packing, or holding of a drug product shall be maintained in a clean and sanitary condition, Any such building shall be free of infestation by rodents, birds, insects, and other vermin (other than laboratory animals). Trash and organic waste matter shall be held and disposed of in a timely and sanitary manner.  (b) There shall be written procedures assigning responsibility for sanitation and describing in sufficient detail the cleaning schedules, methods, equipment, and materials to be used in cleaning the buildings and facilities; such written procedures shall be followed.  (c) There shall be written procedures for use of suitable rodenticides, insecticides, fungicides, fumigating agents, and cleaning and sanitizing agents. Such written procedures shall be designed to prevent the contamination of equipment, components, drug product containers, closures, packaging, labeling materials, or drug products and shall be followed. Rodenticides, insecticides, and fungicides shall not be used unless registered and used in accordance with the Federal Insecticide, Fungicide, and Rodenticide Act (7 U.S.C. 135).  (d) Sanitation procedures shall apply to work performed by contractors or temporary employees as well as work performed by full-time employees during the ordinary course of operations.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['FDA_Description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procedure - 2.2239304783950615\n",
      "cleaning - 2.0509368619420703\n",
      "drug - 1.946237678062678\n",
      "product - 1.704226816239316\n",
      "building - 1.6228804249762585\n",
      "equipment - 1.5274125712250712\n",
      "material - 1.4635425569800566\n",
      "performed - 1.3645765432098766\n",
      "work - 1.3640649691358027\n",
      "fungicide - 1.2933797260802469\n",
      "written - 1.260117901234568\n",
      "Trash - 1.1766111111111113\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(sent1, candidate_pos = ['NOUN', 'PROPN','VERB'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The premises in which a lot or batch of a drug is fabricated, packaged/labelled or stored shall be designed, constructed and maintained in a manner that (a) permits the operations therein to be performed under clean, sanitary and orderly conditions; (b) permits the effective cleaning of all surfaces therein; and (c) prevents the contamination of the drug and the addition of extraneous material to the drug. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['HC_Description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug - 2.1147745181405897\n",
      "permit - 1.6705218253968255\n",
      "fabricated - 1.0468439625850339\n",
      "packagedlabelled - 1.017296343537415\n",
      "stored - 0.9846624149659863\n",
      "surface - 0.9640026927437642\n",
      "prevents - 0.9587913832199545\n",
      "designed - 0.9535535714285713\n",
      "constructed - 0.9503829365079364\n",
      "maintained - 0.9478025793650793\n",
      "manner - 0.9392013888888888\n",
      "contamination - 0.9315880102040816\n"
     ]
    }
   ],
   "source": [
    "tr4w.analyze(sent2, candidate_pos = ['NOUN', 'PROPN','VERB'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source:https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3 = train['fda_desc_cleaned'][1]\n",
    "sent4 = train['hc_desc_cleaned'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buildings in which Type A medicated article(s) are manufactured, processed, packaged, labeled, or held shall be maintained in a clear and orderly manner and shall be of suitable size, construction and location in relation to surroundings to facilitate maintenance and operation for their intended purpose. The building shall:  (a) Provide adequate space for the orderly placement of equipment and materials used in any of the following operations for which they are employed to minimize risk of mixups between different Type A medicated article(s), their components, packaging, or labeling:  (1) The receipt, sampling, control, and storage of components.  (2) Manufacturing and processing operations performed on the Type A medicated article(s).  (3) Packaging and labeling operations.  (4) Storage of containers, packaging materials, labeling, and finished products.  (5) Control laboratory operations.  (b) Provide adequate lighting and ventilation, and when necessary for the intended production or control purposes, adequate screening, dust and temperature controls, to avoid contamination of Type A medicated article(s), and to avoid other conditions unfavorable to the safety, identity, strength, quality, and purity of the raw materials and Type A medicated article(s) before, during, and after production.  (c) Provide for adequate washing, cleaning, toilet, and locker facilities.  Work areas and equipment used for the production of Type A medicated article(s) or for the storage of the components of Type A medicated article(s) shall not be used for the production, mixing or storage of finished or unfinished insecticides, fungicides, rodenticides, or other pesticides or their components unless such materials are recognized as approved drugs intended for use in animal feeds.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['FDA_Description'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "operation - 3.0798367107805316\n",
      "article - 2.8834302194852164\n",
      "Type - 2.608401366351779\n",
      "material - 2.505234934763389\n",
      "component - 2.1401698980798525\n",
      "production - 2.129634272822847\n",
      "Provide - 1.9341169009103094\n",
      "intended - 1.7573903710527676\n",
      "control - 1.6852009975438298\n",
      "storage - 1.670819714022421\n",
      "medicated - 1.65323888405689\n",
      "equipment - 1.4269192391338463\n"
     ]
    }
   ],
   "source": [
    "tr4w.analyze(sent3, candidate_pos = ['NOUN', 'PROPN','VERB'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The premises in which a lot or batch of a drug is fabricated, packaged/labelled or stored shall be designed, constructed and maintained in a manner that (a) permits the operations therein to be performed under clean, sanitary and orderly conditions; (b) permits the effective cleaning of all surfaces therein; and (c) prevents the contamination of the drug and the addition of extraneous material to the drug. '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['HC_Description'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug - 2.1147745181405897\n",
      "permit - 1.6705218253968255\n",
      "fabricated - 1.0468439625850339\n",
      "packagedlabelled - 1.017296343537415\n",
      "stored - 0.9846624149659863\n",
      "surface - 0.9640026927437642\n",
      "prevents - 0.9587913832199545\n",
      "designed - 0.9535535714285713\n",
      "constructed - 0.9503829365079364\n",
      "maintained - 0.9478025793650793\n",
      "manner - 0.9392013888888888\n",
      "contamination - 0.9315880102040816\n"
     ]
    }
   ],
   "source": [
    "tr4w.analyze(sent4, candidate_pos = ['NOUN', 'PROPN','VERB'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
